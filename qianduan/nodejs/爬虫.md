

### 资料
[反爬虫策略及破解方法](https://www.cnblogs.com/micro-chen/p/8676312.html)
如何应对网站反爬虫策略？如何高效地爬大量数据?
> 像一些大型的网站会有反爬虫策略…比如我之前在爬淘宝评论后很快就会被封，大概是短时间爬太多…有什么好的策略吗？比如代理？不过感觉代理也不能稳定吧…

* 故事
```
作者：申玉宝
链接：https://www.zhihu.com/question/28168585/answer/74840535
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

爬虫(Spider)，反爬虫(Anti-Spider)，反反爬虫(Anti-Anti-Spider)，这之间的斗争恢宏壮阔...Day 1小莫想要某站上所有的电影，写了标准的爬虫(基于HttpClient库)，不断地遍历某站的电影列表页面，根据 Html 分析电影名字存进自己的数据库。这个站点的运维小黎发现某个时间段请求量陡增，分析日志发现都是 IP(1.1.1.1)这个用户，并且 useragent 还是 JavaClient1.6 ，基于这两点判断非人类后直接在Nginx 服务器上封杀。Day 2小莫电影只爬了一半，于是也针对性的变换了下策略：1. useragent 模仿百度("Baiduspider...")，2. IP每爬半个小时就换一个IP代理。小黎也发现了对应的变化，于是在 Nginx 上设置了一个频率限制，每分钟超过120次请求的再屏蔽IP。 同时考虑到百度家的爬虫有可能会被误伤，想想市场部门每月几十万的投放，于是写了个脚本，通过 hostname 检查下这个 ip 是不是真的百度家的，对这些 ip 设置一个白名单。Day 3小莫发现了新的限制后，想着我也不急着要这些数据，留给服务器慢慢爬吧，于是修改了代码，随机1-3秒爬一次，爬10次休息10秒，每天只在8-12，18-20点爬，隔几天还休息一下。小黎看着新的日志头都大了，再设定规则不小心会误伤真实用户，于是准备换了一个思路，当3个小时的总请求超过50次的时候弹出一个验证码弹框，没有准确正确输入的话就把 IP 记录进黑名单。Day 4小莫看到验证码有些傻脸了，不过也不是没有办法，先去学习了图像识别（关键词 PIL，tesseract），再对验证码进行了二值化，分词，模式训练之后，识别了小黎的验证码（关于验证码，验证码的识别，验证码的反识别也是一个恢弘壮丽的斗争史，这里先不展开....），之后爬虫又跑了起来。小黎是个不折不挠的好同学，看到验证码被攻破后，和开发同学商量了变化下开发模式，数据并不再直接渲染，而是由前端同学异步获取，并且通过 js 的加密库生成动态的 token，同时加密库再进行混淆（比较重要的步骤的确有网站这样做，参见微博的登陆流程）。Day5混淆过的加密库就没有办法了么？当然不是，可以慢慢调试，找到加密原理，不过小莫不准备用这么耗时耗力的方法，他放弃了基于 HttpClient的爬虫，选择了内置浏览器引擎的爬虫(关键词：PhantomJS，Selenium)，在浏览器引擎中js 加密脚本算出了正确的结果，又一次拿到了对方的数据。小黎：.....爬虫与发爬虫的斗争还在继续。不过实际应用时候，一般大家做到根据 IP 限制频次就结束了，除非很核心的数据，不会再进行更多的验证，毕竟工程的问题一半是成本的问题。至于高效部分，一些 Tips：1.尽量减少请求次数，能抓列表页就不抓详情页2.不要只看 Web 网站，还有 App 和 H5，他们的反爬虫措施一般比较少3.如果真的对性能要求很高，可以考虑多线程(一些成熟的框架如 scrapy都已支持)，甚至分布式

```


### 案例
[爬虫——记一次破解前端加密详细过程](https://blog.csdn.net/lsh19950928/article/details/81585881)
